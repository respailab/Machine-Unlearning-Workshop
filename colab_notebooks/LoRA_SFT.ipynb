{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "b09e3a08-143d-422b-8cfa-2cb993a759a4",
        "id": "pxuw6WJHFYVI"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'trl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2746853823.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSFTConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCollatorForCompletionOnlyLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeftModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepare_model_for_kbit_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'trl'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
        "\n",
        "model_id = 'Qwen/Qwen2.5-3B'\n",
        "modules_to_save=[\"embed_tokens\", \"input_layernorm\", \"post_attention_layernorm\", \"norm\"]\n",
        "lora_alpha = 64\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map='auto',\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir='cache_dir')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    modules_to_save=modules_to_save,\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(base_model)\n",
        "model = PeftModel(base_model, peft_config, adapter_name = 'lora_sft')\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "dataset = load_dataset(\"simplescaling/s1K-1.1\", split=\"train\")\n",
        "\n",
        "def format(x):\n",
        "    x['text'] = f\"\"\"### Question: {x['question']}\\n ### Answer: {x['gemini_attempt']}\"\"\"\n",
        "    return x\n",
        "\n",
        "ds = dataset.map(format)\n",
        "ds_tokenized = ds.map(lambda x : tokenizer(x['text'], padding='longest', truncation=True), batched=True) #return_tensors='pt'\n",
        "ds_tokenized = ds_tokenized.remove_columns(['solution', 'question', 'cot_type', 'source_type', 'metadata', 'gemini_thinking_trajectory', 'gemini_attempt', 'deepseek_thinking_trajectory', 'deepseek_attempt', 'gemini_grade', 'gemini_grade_reason', 'deepseek_grade', 'deepseek_grade_reason', 'text'])\n",
        "\n",
        "response_template = \" ### Answer:\"\n",
        "instruction_template = \"### Question:\"\n",
        "\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template=response_template, instruction_template=instruction_template, tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNOl0_cXSJbc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
        "\n",
        "model_id = 'Qwen/Qwen2.5-3B'\n",
        "modules_to_save=[\"embed_tokens\", \"input_layernorm\", \"post_attention_layernorm\", \"norm\"]\n",
        "lora_alpha = 64\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map='auto',\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir='cache_dir')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    modules_to_save=modules_to_save,\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(base_model)\n",
        "model = PeftModel(base_model, peft_config, adapter_name = 'lora_sft')\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "dataset = load_dataset(\"simplescaling/s1K-1.1\", split=\"train\")\n",
        "\n",
        "def format(x):\n",
        "    x['text'] = f\"\"\"### Question: {x['question']}\\n ### Answer: {x['gemini_attempt']}\"\"\"\n",
        "    return x\n",
        "\n",
        "ds = dataset.map(format)\n",
        "ds_tokenized = ds.map(lambda x : tokenizer(x['text'], padding='longest', truncation=True), batched=True) #return_tensors='pt'\n",
        "ds_tokenized = ds_tokenized.remove_columns(['solution', 'question', 'cot_type', 'source_type', 'metadata', 'gemini_thinking_trajectory', 'gemini_attempt', 'deepseek_thinking_trajectory', 'deepseek_attempt', 'gemini_grade', 'gemini_grade_reason', 'deepseek_grade', 'deepseek_grade_reason', 'text'])\n",
        "\n",
        "response_template = \" ### Answer:\"\n",
        "instruction_template = \"### Question:\"\n",
        "\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template=response_template, instruction_template=instruction_template, tokenizer=tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQr0Wo8LTl9i"
      },
      "outputs": [],
      "source": [
        "training_args = SFTConfig(\n",
        "        output_dir=\"./s1_lora_finetuned\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=1,\n",
        "        max_length=1024,\n",
        "        gradient_accumulation_steps=8,\n",
        "        gradient_checkpointing=True,\n",
        "        save_steps=40,\n",
        "        logging_steps=5,\n",
        "        learning_rate=2e-4, #0.0002\n",
        "        weight_decay=0.01,\n",
        "        max_grad_norm = 0.3,\n",
        "        bf16=True,\n",
        "        # gradient_checkpointing_kwargs = {\"use_reentrant\": False}, #specific to FSDP/DDP\n",
        "        remove_unused_columns=False,\n",
        "        report_to=None,\n",
        "        group_by_length=True,\n",
        "        lr_scheduler_type='cosine',\n",
        "        optim=\"adamw_torch\",\n",
        "    )\n",
        "trainer = SFTTrainer(\n",
        "        model,\n",
        "        data_collator=collator,\n",
        "        peft_config=peft_config,\n",
        "        args=training_args,\n",
        "        train_dataset=ds_tokenized,\n",
        "        processing_class=tokenizer\n",
        "    )\n",
        "\n",
        "trainer.train()\n",
        "trainer.model.save_pretrained('math_longcot_finetuned_qwen_base')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFl5rwbXVexh"
      },
      "outputs": [],
      "source": [
        "!pip3 install trl"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}