{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import required libraries"
      ],
      "metadata": {
        "id": "k6V5ELtBEg7V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHbZfSRX5AD-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as tt\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "torch.manual_seed(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting drive to load saved model\n",
        "Saved Model - [Google Drive](https://drive.google.com/file/d/1QkEXtdRpYvDdRUi4LtF9OZen8kiayqVo/view?usp=drive_link)\n",
        "\n",
        "\n",
        "> You can download the model from the above link and upload it to colab files using the cell below\n",
        "\n"
      ],
      "metadata": {
        "id": "wkcWpHocEmxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "id": "vpdzY8oAFYYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Or you can copy the model to your drive and use it\n",
        "\n"
      ],
      "metadata": {
        "id": "-Eiz0uRUGDzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZrqQTIeHOwm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper functions"
      ],
      "metadata": {
        "id": "NeG5v7UGGOsl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "def training_step(model, batch):\n",
        "    images, labels = batch\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    out = model(images)\n",
        "    loss = F.cross_entropy(out, labels)\n",
        "    return loss\n",
        "\n",
        "def validation_step(model, batch):\n",
        "    images, labels = batch\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    out = model(images)\n",
        "    loss = F.cross_entropy(out, labels)\n",
        "    acc = accuracy(out, labels)\n",
        "    return {'Loss': loss.detach(), 'Acc': acc}\n",
        "\n",
        "def validation_epoch_end(model, outputs):\n",
        "    batch_losses = [x['Loss'] for x in outputs]\n",
        "    epoch_loss = torch.stack(batch_losses).mean()\n",
        "    batch_accs = [x['Acc'] for x in outputs]\n",
        "    epoch_acc = torch.stack(batch_accs).mean()\n",
        "    return {'Loss': epoch_loss.item(), 'Acc': epoch_acc.item()}\n",
        "\n",
        "def epoch_end(model, epoch, result):\n",
        "    print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "        epoch, result['lrs'][-1], result['train_loss'], result['Loss'], result['Acc']))\n",
        "\n",
        "def distance(model,model0):\n",
        "    distance=0\n",
        "    normalization=0\n",
        "    for (k, p), (k0, p0) in zip(model.named_parameters(), model0.named_parameters()):\n",
        "        space='  ' if 'bias' in k else ''\n",
        "        current_dist=(p.data0-p0.data0).pow(2).sum().item()\n",
        "        current_norm=p.data0.pow(2).sum().item()\n",
        "        distance+=current_dist\n",
        "        normalization+=current_norm\n",
        "    print(f'Distance: {np.sqrt(distance)}')\n",
        "    print(f'Normalized Distance: {1.0*np.sqrt(distance/normalization)}')\n",
        "    return 1.0*np.sqrt(distance/normalization)"
      ],
      "metadata": {
        "id": "TtmvI93H8_ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [validation_step(model, batch) for batch in val_loader]\n",
        "    return validation_epoch_end(model, outputs)\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n",
        "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "\n",
        "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []\n",
        "        for batch in train_loader:\n",
        "            loss = training_step(model, batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "\n",
        "            if grad_clip:\n",
        "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            lrs.append(get_lr(optimizer))\n",
        "\n",
        "\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        epoch_end(model, epoch, result)\n",
        "        history.append(result)\n",
        "        sched.step(result['Loss'])\n",
        "    return history"
      ],
      "metadata": {
        "id": "WRTEEa9G9EVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset download and preprocessing"
      ],
      "metadata": {
        "id": "H9JQUp7GGYIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dowload the dataset\n",
        "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
        "download_url(dataset_url, '.')\n",
        "\n",
        "# Extract from archive\n",
        "with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
        "    tar.extractall(path='./data')\n",
        "\n",
        "# Look into the data directory\n",
        "data_dir = './data/cifar10'\n",
        "print(os.listdir(data_dir))\n",
        "classes = os.listdir(data_dir + \"/train\")\n",
        "print(classes)"
      ],
      "metadata": {
        "id": "naZs114k9Kj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = tt.Compose([\n",
        "    tt.ToTensor(),\n",
        "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = tt.Compose([\n",
        "    tt.ToTensor(),\n",
        "    tt.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "metadata": {
        "id": "quovwVoc9NFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = ImageFolder(data_dir+'/train', transform_train)\n",
        "valid_ds = ImageFolder(data_dir+'/test', transform_test)"
      ],
      "metadata": {
        "id": "GGzil9OE9QJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "W3NehY6w9Ss2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training configuration"
      ],
      "metadata": {
        "id": "IdYuwQdYGjQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "model = resnet18(num_classes = 10).to(device = device)\n",
        "\n",
        "epochs = 40\n",
        "max_lr = 0.01\n",
        "grad_clip = 0.1\n",
        "weight_decay = 1e-4\n",
        "opt_func = torch.optim.Adam"
      ],
      "metadata": {
        "id": "Au7LRMbv9Umq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and save the model"
      ],
      "metadata": {
        "id": "HuF2EQpyGmQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# history = fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl,\n",
        "#                              grad_clip=grad_clip,\n",
        "#                              weight_decay=weight_decay,\n",
        "#                              opt_func=opt_func)\n",
        "\n",
        "# torch.save(model.state_dict(), \"ResNET18_CIFAR10_ALL_CLASSES.pt\")"
      ],
      "metadata": {
        "id": "YaooNqPz9b8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the saved model (from files or drive)"
      ],
      "metadata": {
        "id": "Gip-4fXEGpxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_state_dict(torch.load(\"/content/drive/MyDrive/Machine-Unlearning-Workshop 2025/ResNET18_CIFAR10_ALL_CLASSES.pt\"))\n",
        "model.load_state_dict(torch.load(\"/content/ResNET18_CIFAR10_ALL_CLASSES.pt\"))\n",
        "original_model_history = [evaluate(model, valid_dl)]\n",
        "original_model_history"
      ],
      "metadata": {
        "id": "P1PN7htP9sdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create noise for unlearning step"
      ],
      "metadata": {
        "id": "WcaRSbZXHCjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the noise structure\n",
        "class Noise(nn.Module):\n",
        "    def __init__(self, *dim):\n",
        "        super().__init__()\n",
        "        self.noise = torch.nn.Parameter(torch.randn(*dim), requires_grad = True)\n",
        "\n",
        "    def forward(self):\n",
        "        return self.noise"
      ],
      "metadata": {
        "id": "Fj1_WPNn-fa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select classes to unlearn"
      ],
      "metadata": {
        "id": "dCJRME2LHGaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list of all classes\n",
        "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "# classes which are required to un-learn\n",
        "classes_to_forget = [0, 2]"
      ],
      "metadata": {
        "id": "t7PJGj-H-gCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# classwise list of samples\n",
        "num_classes = 10\n",
        "classwise_train = {}\n",
        "for i in range(num_classes):\n",
        "    classwise_train[i] = []\n",
        "\n",
        "for img, label in train_ds:\n",
        "    classwise_train[label].append((img, label))\n",
        "\n",
        "classwise_test = {}\n",
        "for i in range(num_classes):\n",
        "    classwise_test[i] = []\n",
        "\n",
        "for img, label in valid_ds:\n",
        "    classwise_test[label].append((img, label))"
      ],
      "metadata": {
        "id": "ok5TDXr1-lZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting some samples from retain classes\n",
        "num_samples_per_class = 1000\n",
        "\n",
        "retain_samples = []\n",
        "for i in range(len(classes)):\n",
        "    if classes[i] not in classes_to_forget:\n",
        "        retain_samples += classwise_train[i][:num_samples_per_class]"
      ],
      "metadata": {
        "id": "wcRpHrNT-ob4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retain validation set\n",
        "retain_valid = []\n",
        "for cls in range(num_classes):\n",
        "    if cls not in classes_to_forget:\n",
        "        for img, label in classwise_test[cls]:\n",
        "            retain_valid.append((img, label))\n",
        "\n",
        "# forget validation set\n",
        "forget_valid = []\n",
        "for cls in range(num_classes):\n",
        "    if cls in classes_to_forget:\n",
        "        for img, label in classwise_test[cls]:\n",
        "            forget_valid.append((img, label))\n",
        "\n",
        "forget_valid_dl = DataLoader(forget_valid, batch_size, num_workers=2, pin_memory=True)\n",
        "retain_valid_dl = DataLoader(retain_valid, batch_size*2, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "ZeVKNf8E-q1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the model\n",
        "model = resnet18(num_classes = 10).to(device = device)\n",
        "# model.load_state_dict(torch.load(\"/content/drive/MyDrive/Machine-Unlearning-Workshop 2025/ResNET18_CIFAR10_ALL_CLASSES.pt\"))\n",
        "model.load_state_dict(torch.load(\"/content/ResNET18_CIFAR10_ALL_CLASSES.pt\"))"
      ],
      "metadata": {
        "id": "VuvX5mPg-vHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the noise"
      ],
      "metadata": {
        "id": "PSle3VfmH962"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "noises = {}\n",
        "for cls in classes_to_forget:\n",
        "    print(\"Optiming loss for class {}\".format(cls))\n",
        "    noises[cls] = Noise(batch_size, 3, 32, 32).cuda()\n",
        "    opt = torch.optim.Adam(noises[cls].parameters(), lr = 0.1)\n",
        "\n",
        "    num_epochs = 5\n",
        "    num_steps = 8\n",
        "    class_label = cls\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = []\n",
        "        for batch in range(num_steps):\n",
        "            inputs = noises[cls]()\n",
        "            labels = torch.zeros(batch_size).cuda()+class_label\n",
        "            outputs = model(inputs)\n",
        "            loss = -F.cross_entropy(outputs, labels.long()) + 0.1*torch.mean(torch.sum(torch.square(inputs), [1, 2, 3]))\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss.append(loss.cpu().detach().numpy())\n",
        "        print(\"Loss: {}\".format(np.mean(total_loss)))"
      ],
      "metadata": {
        "id": "qpuWoOQ9-zYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Impair step"
      ],
      "metadata": {
        "id": "BnnJuFqNIGED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "batch_size = 256\n",
        "noisy_data = []\n",
        "num_batches = 20\n",
        "class_num = 0\n",
        "\n",
        "for cls in classes_to_forget:\n",
        "    for i in range(num_batches):\n",
        "        batch = noises[cls]().cpu().detach()\n",
        "        for i in range(batch[0].size(0)):\n",
        "            noisy_data.append((batch[i], torch.tensor(class_num)))\n",
        "\n",
        "other_samples = []\n",
        "for i in range(len(retain_samples)):\n",
        "    other_samples.append((retain_samples[i][0].cpu(), torch.tensor(retain_samples[i][1])))\n",
        "noisy_data += other_samples\n",
        "noisy_loader = torch.utils.data.DataLoader(noisy_data, batch_size=256, shuffle = True)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.02)\n",
        "\n",
        "\n",
        "for epoch in range(1):\n",
        "    model.train(True)\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0\n",
        "    for i, data in enumerate(noisy_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.cuda(),torch.tensor(labels).cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        out = torch.argmax(outputs.detach(),dim=1)\n",
        "        assert out.shape==labels.shape\n",
        "        running_acc += (labels==out).sum().item()\n",
        "    print(f\"Train loss {epoch+1}: {running_loss/len(train_ds)},Train Acc:{running_acc*100/len(train_ds)}%\")"
      ],
      "metadata": {
        "id": "pwrUkxsW-2dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance of the model of retain and forget class after impair step"
      ],
      "metadata": {
        "id": "-vgUKQ0GIYjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Performance of Standard Forget Model on Forget Class\")\n",
        "impair_step_history_f = [evaluate(model, forget_valid_dl)]\n",
        "print(\"Accuracy: {}\".format(impair_step_history_f[0][\"Acc\"]*100))\n",
        "print(\"Loss: {}\".format(impair_step_history_f[0][\"Loss\"]))\n",
        "\n",
        "print(\"Performance of Standard Forget Model on Retain Class\")\n",
        "impair_step_history_r = [evaluate(model, retain_valid_dl)]\n",
        "print(\"Accuracy: {}\".format(impair_step_history_r[0][\"Acc\"]*100))\n",
        "print(\"Loss: {}\".format(impair_step_history_r[0][\"Loss\"]))"
      ],
      "metadata": {
        "id": "6WD_pBdU_Avy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repair step"
      ],
      "metadata": {
        "id": "EtymRf7zIfPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "heal_loader = torch.utils.data.DataLoader(other_samples, batch_size=256, shuffle = True)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
        "\n",
        "\n",
        "for epoch in range(1):\n",
        "    model.train(True)\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0\n",
        "    for i, data in enumerate(heal_loader):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.cuda(),torch.tensor(labels).cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = F.cross_entropy(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        out = torch.argmax(outputs.detach(),dim=1)\n",
        "        assert out.shape==labels.shape\n",
        "        running_acc += (labels==out).sum().item()\n",
        "    print(f\"Train loss {epoch+1}: {running_loss/len(train_ds)},Train Acc:{running_acc*100/len(train_ds)}%\")"
      ],
      "metadata": {
        "id": "Wt6pNbM__EPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance of the model on retain and forget class after repair step"
      ],
      "metadata": {
        "id": "fCDnfnAlIig4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Performance of Standard Forget Model on Forget Class\")\n",
        "repair_step_history_f = [evaluate(model, forget_valid_dl)]\n",
        "print(\"Accuracy: {}\".format(repair_step_history_f[0][\"Acc\"]*100))\n",
        "print(\"Loss: {}\".format(repair_step_history_f[0][\"Loss\"]))\n",
        "\n",
        "print(\"Performance of Standard Forget Model on Retain Class\")\n",
        "repair_step_history_r = [evaluate(model, retain_valid_dl)]\n",
        "print(\"Accuracy: {}\".format(repair_step_history_r[0][\"Acc\"]*100))\n",
        "print(\"Loss: {}\".format(repair_step_history_r[0][\"Loss\"]))"
      ],
      "metadata": {
        "id": "hcgDKBlf_HO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original model history:\")\n",
        "print(original_model_history)\n",
        "print(\"\\nPerformance on Forget Class after Impair step:\")\n",
        "print(impair_step_history_f)\n",
        "print(\"\\nPerformance on Retain Class after Impair step:\")\n",
        "print(impair_step_history_r)\n",
        "print(\"\\nPerformance on Forget Class after Repair step:\")\n",
        "print(repair_step_history_f)\n",
        "print(\"\\nPerformance on Retain Class after Repair step:\")\n",
        "print(repair_step_history_r)"
      ],
      "metadata": {
        "id": "PKvRp_WiLKi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e85b42a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Extracting accuracy and loss values\n",
        "forget_accs = [original_model_history[0]['Acc'], impair_step_history_f[0]['Acc'], repair_step_history_f[0]['Acc']]\n",
        "retain_accs = [original_model_history[0]['Acc'], impair_step_history_r[0]['Acc'], repair_step_history_r[0]['Acc']]\n",
        "\n",
        "forget_losses = [original_model_history[0]['Loss'], impair_step_history_f[0]['Loss'], repair_step_history_f[0]['Loss']]\n",
        "retain_losses = [original_model_history[0]['Loss'], impair_step_history_r[0]['Loss'], repair_step_history_r[0]['Loss']]\n",
        "\n",
        "labels = ['Before Unlearning', 'After Impair', 'After Repair']\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "# Create a figure with two subplots (one for accuracy, one for loss)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Accuracy plot\n",
        "rects1_acc = ax1.bar(x - width/2, [acc * 100 for acc in forget_accs], width, label='Forget Class', color='#1f77b4', alpha=0.8)\n",
        "rects2_acc = ax1.bar(x + width/2, [acc * 100 for acc in retain_accs], width, label='Retain Class', color='#ff7f0e', alpha=0.8)\n",
        "\n",
        "ax1.set_ylabel('Accuracy (%)')\n",
        "ax1.set_title('Model Accuracy')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(labels)\n",
        "ax1.legend()\n",
        "ax1.set_ylim(0, 100) # Set y-axis limit for accuracy\n",
        "\n",
        "# Add percentage labels to accuracy bars\n",
        "def autolabel_acc(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax1.annotate('%.2f%%' % height,\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel_acc(rects1_acc)\n",
        "autolabel_acc(rects2_acc)\n",
        "\n",
        "# Loss plot\n",
        "rects1_loss = ax2.bar(x - width/2, forget_losses, width, label='Forget Class', color='#1f77b4', alpha=0.8)\n",
        "rects2_loss = ax2.bar(x + width/2, retain_losses, width, label='Retain Class', color='#ff7f0e', alpha=0.8)\n",
        "\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_title('Model Loss')\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(labels)\n",
        "ax2.legend()\n",
        "ax2.set_ylim(0, max(max(forget_losses), max(retain_losses)) * 1.1) # Set y-axis limit for loss\n",
        "\n",
        "# Add loss values labels to loss bars\n",
        "def autolabel_loss(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax2.annotate('%.2f' % height,\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "autolabel_loss(rects1_loss)\n",
        "autolabel_loss(rects2_loss)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2PFd0APEMCG_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}