{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b72a11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/visionintelligence/Vikram/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import wget\n",
    "import zipfile\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn import functional as F\n",
    "from utils import clean_text\n",
    "from models import LSTMnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924a107a",
   "metadata": {},
   "source": [
    "## Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a44827b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, batch, device):\n",
    "    sent1, sent2, labels = batch \n",
    "    sent1, sent2, labels = sent1.to(device), sent2.to(device), labels.to(device)\n",
    "    out, *_  = model(sent1, sent2)                  # Generate predictions\n",
    "    loss= F.mse_loss(out, labels) # Calculate loss\n",
    "    return loss\n",
    "\n",
    "def validation_step(model, batch, device):\n",
    "    sent1, sent2, labels = batch \n",
    "    sent1, sent2, labels = sent1.to(device), sent2.to(device), labels.to(device)\n",
    "    out, *_  = model(sent1, sent2)                    # Generate predictions\n",
    "    loss= F.mse_loss(out, labels)   # Calculate loss\n",
    "    return {'Loss': loss.detach()}\n",
    "\n",
    "def validation_epoch_end(model, outputs):\n",
    "    batch_losses = [x['Loss'] for x in outputs]\n",
    "    epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "    return {'Loss': epoch_loss.item()}\n",
    "\n",
    "def epoch_end(model, epoch, result):\n",
    "    print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}\".format(\n",
    "        epoch, result['lrs'][-1], result['train_loss'], result['Loss']))\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_df, device, batch_size = 256):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    \n",
    "    num_steps = len(val_df)//batch_size\n",
    "    \n",
    "    for i in range(num_steps):\n",
    "        sent1 = torch.tensor(np.stack(val_df.iloc[i*batch_size:(i+1)*batch_size]['sentence1'])).float()\n",
    "        sent2 = torch.tensor(np.stack(val_df.iloc[i*batch_size:(i+1)*batch_size]['sentence2'])).float()\n",
    "        labels = torch.tensor(val_df.iloc[i*batch_size:(i+1)*batch_size]['score'].values)\n",
    "        batch = (sent1, sent2, labels)\n",
    "\n",
    "        outputs.append(validation_step(model, batch, device))\n",
    "        \n",
    "    if len(val_df)%batch_size != 0:\n",
    "        sent1 = torch.tensor(np.stack(val_df.iloc[num_steps*batch_size:]['sentence1'])).float()\n",
    "        sent2 = torch.tensor(np.stack(val_df.iloc[num_steps*batch_size:]['sentence2'])).float()\n",
    "        labels = torch.tensor(val_df.iloc[num_steps*batch_size:]['score'].values)\n",
    "        batch = (sent1, sent2, labels)\n",
    "\n",
    "        outputs.append(validation_step(model, batch, device))\n",
    "        \n",
    "    return validation_epoch_end(model, outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs,  model, train_df, val_df, device, save_path, batch_size = 256):\n",
    "    best_loss = np.inf\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "    num_steps = len(train_df)//batch_size\n",
    "    \n",
    "    #for \n",
    "    for epoch in range(epochs): \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for i in range(num_steps):\n",
    "            sent1 = torch.tensor(np.stack(train_df.iloc[i*batch_size:(i+1)*batch_size]['sentence1'])).float()\n",
    "            sent2 = torch.tensor(np.stack(train_df.iloc[i*batch_size:(i+1)*batch_size]['sentence2'])).float()\n",
    "            labels = torch.tensor(train_df.iloc[i*batch_size:(i+1)*batch_size]['score'].values).float()\n",
    "            batch = (sent1, sent2, labels)\n",
    "            loss = training_step(model, batch, device)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            lrs.append(get_lr(optimizer))\n",
    "        if len(train_df)%batch_size != 0:\n",
    "            sent1 = torch.tensor(np.stack(train_df.iloc[num_steps*batch_size:]['sentence1'])).float()\n",
    "            sent2 = torch.tensor(np.stack(train_df.iloc[num_steps*batch_size:]['sentence2'])).float()\n",
    "            labels = torch.tensor(train_df.iloc[num_steps*batch_size:]['score'].values).float()\n",
    "            batch = (sent1, sent2, labels)\n",
    "            loss = training_step(model, batch, device)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            lrs.append(get_lr(optimizer))\n",
    "            \n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_df, device, batch_size)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        epoch_end(model, epoch, result)\n",
    "        history.append(result)\n",
    "        sched.step(result['Loss'])\n",
    "        if best_loss > result['Loss']:\n",
    "            best_loss = result['Loss']\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0530992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_one_finetune_cycle(epochs,  model, train_df, val_df, lr, device, save_path, batch_size = 256):\n",
    "    best_loss = np.inf\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "    num_steps = len(train_df)//batch_size\n",
    "    \n",
    "    #for \n",
    "    for epoch in range(epochs): \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for i in range(num_steps):\n",
    "            sent1 = torch.tensor(np.stack(train_df.iloc[i*batch_size:(i+1)*batch_size]['sentence1'])).float()\n",
    "            sent2 = torch.tensor(np.stack(train_df.iloc[i*batch_size:(i+1)*batch_size]['sentence2'])).float()\n",
    "            labels = torch.tensor(train_df.iloc[i*batch_size:(i+1)*batch_size]['score'].values).float()\n",
    "            batch = (sent1, sent2, labels)\n",
    "            loss = training_step(model, batch, device)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            lrs.append(get_lr(optimizer))\n",
    "        if len(train_df)%batch_size != 0:\n",
    "            sent1 = torch.tensor(np.stack(train_df.iloc[num_steps*batch_size:]['sentence1'])).float()\n",
    "            sent2 = torch.tensor(np.stack(train_df.iloc[num_steps*batch_size:]['sentence2'])).float()\n",
    "            labels = torch.tensor(train_df.iloc[num_steps*batch_size:]['score'].values).float()\n",
    "            batch = (sent1, sent2, labels)\n",
    "            loss = training_step(model, batch, device)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            lrs.append(get_lr(optimizer))\n",
    "            \n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_df, device, batch_size)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        epoch_end(model, epoch, result)\n",
    "        history.append(result)\n",
    "        sched.step(result['Loss'])\n",
    "        if best_loss > result['Loss']:\n",
    "            best_loss = result['Loss']\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b683e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(x):\n",
    "        \"\"\"\n",
    "        Taken from https://github.com/szagoruyko/attention-transfer\n",
    "        :param x = activations\n",
    "        \"\"\"\n",
    "        return F.normalize(x.pow(2).mean(1).view(x.size(0), -1))\n",
    "\n",
    "\n",
    "def attention_diff(x, y):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/szagoruyko/attention-transfer\n",
    "    :param x = activations\n",
    "    :param y = activations\n",
    "    \"\"\"\n",
    "    return (attention(x) - attention(y)).pow(2).mean()\n",
    "\n",
    "\n",
    "\n",
    "def forget_loss(model_output, model_activations, proxy_output, proxy_activations, mask):\n",
    "\n",
    "    loss = F.mse_loss(model_output[mask], proxy_output[mask])\n",
    "    if AT_beta > 0:\n",
    "        at_loss = 0\n",
    "        for i in range(len(proxy_activations)):\n",
    "            at_loss = at_loss + AT_beta * attention_diff(model_activations[i][mask], proxy_activations[i][mask])\n",
    "    else:\n",
    "        at_loss = 0\n",
    "\n",
    "    total_loss = loss + at_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "def fit_one_forget_cycle(epochs,  model, proxy_model, train_df, val_df, lr, device, save_path, batch_size = 256):\n",
    "    best_loss = np.inf\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "    sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "    num_steps = len(train_df)//batch_size\n",
    "    for epoch in range(epochs): \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        #for batch in train_loader:\n",
    "        for i in range(num_steps):\n",
    "            sent1 = torch.tensor(np.stack(train_df.iloc[i*batch_size:(i+1)*batch_size]['sentence1'])).float()\n",
    "            sent2 = torch.tensor(np.stack(train_df.iloc[i*batch_size:(i+1)*batch_size]['sentence2'])).float()\n",
    "            labels = torch.tensor(train_df.iloc[i*batch_size:(i+1)*batch_size]['score'].values).float()\n",
    "            ulabels = torch.tensor(train_df.iloc[i*batch_size:(i+1)*batch_size]['forget'].values)\n",
    "            \n",
    "            sent1, sent2, labels, ulabels = sent1.to(device), sent2.to(device), labels.to(device), ulabels.to(device)\n",
    "            \n",
    "            model_out, *model_activations = model(sent1, sent2)\n",
    "            with torch.no_grad():\n",
    "                proxy_out, *proxy_activations = proxy_model(sent1, sent2)\n",
    "                \n",
    "            \n",
    "            label_loss = 0\n",
    "            if ulabels.sum() < len(ulabels):\n",
    "                mask = (ulabels == 0)\n",
    "                r_model_out = model_out[mask]\n",
    "                r_labels = labels[mask]\n",
    "                label_loss = F.mse_loss(r_model_out, r_labels)\n",
    "            \n",
    "            proxy_loss = 0\n",
    "            if ulabels.sum() > 0:\n",
    "                mask = (ulabels == 1)\n",
    "                proxy_loss = forget_loss(model_out, model_activations, proxy_out, proxy_activations, mask)\n",
    "            \n",
    "            coeff = ulabels.sum()/len(ulabels)\n",
    "            loss = coeff*proxy_loss + (1-coeff)*label_loss\n",
    "            \n",
    "            ######\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            lrs.append(get_lr(optimizer))\n",
    "            \n",
    "        if len(train_df)%batch_size != 0:\n",
    "            sent1 = torch.tensor(np.stack(train_df.iloc[num_steps*batch_size:]['sentence1'])).float()\n",
    "            sent2 = torch.tensor(np.stack(train_df.iloc[num_steps*batch_size:]['sentence2'])).float()\n",
    "            labels = torch.tensor(train_df.iloc[num_steps*batch_size:]['score'].values).float()\n",
    "            ulabels = torch.tensor(train_df.iloc[num_steps*batch_size:]['forget'].values)\n",
    "            \n",
    "            sent1, sent2, labels, ulabels = sent1.to(device), sent2.to(device), labels.to(device), ulabels.to(device)\n",
    "            \n",
    "            model_out, *model_activations = model(sent1, sent2)\n",
    "            with torch.no_grad():\n",
    "                proxy_out, *proxy_activations = proxy_model(sent1, sent2)\n",
    "                \n",
    "            \n",
    "            label_loss = 0\n",
    "            if ulabels.sum() < len(ulabels):\n",
    "                mask = (ulabels == 0)\n",
    "                r_model_out = model_out[mask]\n",
    "                r_labels = labels[mask]\n",
    "                label_loss = F.mse_loss(r_model_out, r_labels)\n",
    "            \n",
    "            proxy_loss = 0\n",
    "            if ulabels.sum() > 0:\n",
    "                mask = (ulabels == 1)\n",
    "                proxy_loss = forget_loss(model_out, model_activations, proxy_out, proxy_activations, mask)\n",
    "            \n",
    "            coeff = ulabels.sum()/len(ulabels)\n",
    "            loss = coeff*proxy_loss + (1-coeff)*label_loss\n",
    "            \n",
    "            ######\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            lrs.append(get_lr(optimizer))\n",
    "            \n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_df, device)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        epoch_end(model, epoch, result)\n",
    "        history.append(result)\n",
    "        #sched.step(result['Loss'])\n",
    "        #if best_loss > result['Loss']:\n",
    "        #    best_loss = result['Loss']\n",
    "        #    torch.save(model.state_dict(), save_path)\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c619d91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding_dimension = 300\n",
    "\n",
    "def text_embed(words):\n",
    "    \n",
    "    unknown_indices = []\n",
    "    mean = np.zeros(text_embedding_dimension)\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        if words[i] in embeddings_index_300 and embeddings_index_300[ words[i] ].shape == (300, ):\n",
    "            words[i] = embeddings_index_300[ words[i] ]\n",
    "            mean += words[i]\n",
    "        else:\n",
    "            unknown_indices.append(i)\n",
    "            \n",
    "    mean /= max(len(words)-len(unknown_indices), 1)\n",
    "    \n",
    "    # unknown words in the text are represented using the mean of the known words\n",
    "    for i in unknown_indices:\n",
    "        words[i] = mean\n",
    "    return words\n",
    "\n",
    "def pad(x, max_len = 10):\n",
    "    if len(x) >= max_len:\n",
    "        return x[:10]\n",
    "    zeros = [np.zeros(text_embedding_dimension)]*(max_len - len(x))\n",
    "    return zeros + x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c92dd4",
   "metadata": {},
   "source": [
    "## Get GLOVE Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa7b4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and extracting GloVe word embeddings...\n",
      "\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading and extracting GloVe word embeddings...\")\n",
    "data_file = \"./glove.840B.300d.zip\"\n",
    "wget.download(\"http://nlp.stanford.edu/data/glove.840B.300d.zip\", out=data_file)\n",
    "with zipfile.ZipFile(data_file) as zip_ref:\n",
    "    zip_ref.extractall('./glove')\n",
    "os.remove(data_file)\n",
    "print(\"\\nCompleted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24ba7212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3686965/1901847276.py:7: DeprecationWarning: string or file could not be read to its end due to unmatched data; this will raise a ValueError in the future.\n",
      "  coefs = np.fromstring(coefs, \"f\", sep=\" \")\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file = \"./glove/glove.840B.300d.txt\"\n",
    "\n",
    "embeddings_index_300 = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index_300[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d96820e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2195884 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print(\"Found %s word vectors.\" % len(embeddings_index_300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b826fa",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "744d72db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./stsb_data/train_new.tsv\", sep='\\t', on_bad_lines='skip')\n",
    "val_df = pd.read_csv(\"./stsb_data/dev_new.tsv\", sep='\\t', on_bad_lines='skip')\n",
    "test_df = pd.read_csv(\"./stsb_data/test_new.tsv\", sep='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baa4f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(subset=['score'], inplace=True)\n",
    "val_df.dropna(subset=['score'], inplace=True)\n",
    "test_df.dropna(subset=['score'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16b33a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sentence1'] = train_df['sentence1'].apply(lambda x: clean_text(x))\n",
    "train_df['sentence2'] = train_df['sentence2'].apply(lambda x: clean_text(x))\n",
    "\n",
    "val_df['sentence1'] = val_df['sentence1'].apply(lambda x: clean_text(x))\n",
    "val_df['sentence2'] = val_df['sentence2'].apply(lambda x: clean_text(x))\n",
    "\n",
    "test_df['sentence1'] = test_df['sentence1'].apply(lambda x: clean_text(x))\n",
    "test_df['sentence2'] = test_df['sentence2'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcbbe74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sentence1'] = train_df['sentence1'].apply(lambda words: text_embed(words))\n",
    "train_df['sentence2'] = train_df['sentence2'].apply(lambda words: text_embed(words))\n",
    "\n",
    "val_df['sentence1'] = val_df['sentence1'].apply(lambda words: text_embed(words))\n",
    "val_df['sentence2'] = val_df['sentence2'].apply(lambda words: text_embed(words))\n",
    "\n",
    "test_df['sentence1'] = test_df['sentence1'].apply(lambda words: text_embed(words))\n",
    "test_df['sentence2'] = test_df['sentence2'].apply(lambda words: text_embed(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac57e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['sentence1'] = train_df['sentence1'].apply(lambda words: pad(words))\n",
    "train_df['sentence2'] = train_df['sentence2'].apply(lambda words: pad(words))\n",
    "\n",
    "val_df['sentence1'] = val_df['sentence1'].apply(lambda words: pad(words))\n",
    "val_df['sentence2'] = val_df['sentence2'].apply(lambda words: pad(words))\n",
    "\n",
    "test_df['sentence1'] = test_df['sentence1'].apply(lambda words: pad(words))\n",
    "test_df['sentence2'] = test_df['sentence2'].apply(lambda words: pad(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff0a35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac = 1, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b997c",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20581b40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.01000, train_loss: 3.7514, val_loss: 2.1620\n",
      "Epoch [1], last_lr: 0.01000, train_loss: 2.2661, val_loss: 2.0303\n",
      "Epoch [2], last_lr: 0.01000, train_loss: 2.2280, val_loss: 1.9903\n",
      "Epoch [3], last_lr: 0.01000, train_loss: 2.1673, val_loss: 1.9712\n",
      "Epoch [4], last_lr: 0.01000, train_loss: 2.0814, val_loss: 1.9476\n",
      "Epoch [5], last_lr: 0.01000, train_loss: 1.9387, val_loss: 1.9810\n",
      "Epoch [6], last_lr: 0.01000, train_loss: 1.7108, val_loss: 2.0152\n",
      "Epoch [7], last_lr: 0.01000, train_loss: 1.5121, val_loss: 2.0804\n",
      "Epoch [8], last_lr: 0.01000, train_loss: 1.3200, val_loss: 2.1425\n",
      "Epoch [9], last_lr: 0.01000, train_loss: 1.2409, val_loss: 2.2455\n",
      "Epoch [10], last_lr: 0.01000, train_loss: 1.3078, val_loss: 3.0867\n",
      "Epoch [11], last_lr: 0.01000, train_loss: 1.7610, val_loss: 2.3519\n",
      "Epoch [12], last_lr: 0.01000, train_loss: 1.3685, val_loss: 2.1034\n",
      "Epoch [13], last_lr: 0.01000, train_loss: 0.7959, val_loss: 2.3659\n",
      "Epoch [14], last_lr: 0.01000, train_loss: 0.6454, val_loss: 2.4677\n",
      "Epoch [15], last_lr: 0.01000, train_loss: 0.5746, val_loss: 2.4120\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch [16], last_lr: 0.00100, train_loss: 0.4698, val_loss: 2.3867\n",
      "Epoch [17], last_lr: 0.00100, train_loss: 0.4407, val_loss: 2.4148\n",
      "Epoch [18], last_lr: 0.00100, train_loss: 0.4346, val_loss: 2.3970\n",
      "Epoch [19], last_lr: 0.00100, train_loss: 0.4076, val_loss: 2.4035\n",
      "Epoch [20], last_lr: 0.00100, train_loss: 0.4085, val_loss: 2.4092\n",
      "Epoch [21], last_lr: 0.00100, train_loss: 0.3912, val_loss: 2.4234\n",
      "Epoch [22], last_lr: 0.00100, train_loss: 0.3870, val_loss: 2.4152\n",
      "Epoch [23], last_lr: 0.00100, train_loss: 0.3731, val_loss: 2.4115\n",
      "Epoch [24], last_lr: 0.00100, train_loss: 0.3774, val_loss: 2.4153\n",
      "Epoch [25], last_lr: 0.00100, train_loss: 0.3694, val_loss: 2.4129\n",
      "Epoch [26], last_lr: 0.00100, train_loss: 0.3600, val_loss: 2.4117\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch [27], last_lr: 0.00010, train_loss: 0.3481, val_loss: 2.4155\n",
      "Epoch [28], last_lr: 0.00010, train_loss: 0.3343, val_loss: 2.4162\n",
      "Epoch [29], last_lr: 0.00010, train_loss: 0.3467, val_loss: 2.4154\n",
      "Epoch [30], last_lr: 0.00010, train_loss: 0.3555, val_loss: 2.4142\n",
      "Epoch [31], last_lr: 0.00010, train_loss: 0.3464, val_loss: 2.4153\n",
      "Epoch [32], last_lr: 0.00010, train_loss: 0.3443, val_loss: 2.4130\n",
      "Epoch [33], last_lr: 0.00010, train_loss: 0.3417, val_loss: 2.4131\n",
      "Epoch [34], last_lr: 0.00010, train_loss: 0.3423, val_loss: 2.4147\n",
      "Epoch [35], last_lr: 0.00010, train_loss: 0.3409, val_loss: 2.4150\n",
      "Epoch [36], last_lr: 0.00010, train_loss: 0.3507, val_loss: 2.4144\n",
      "Epoch [37], last_lr: 0.00010, train_loss: 0.3405, val_loss: 2.4174\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch [38], last_lr: 0.00001, train_loss: 0.3391, val_loss: 2.4176\n",
      "Epoch [39], last_lr: 0.00001, train_loss: 0.3467, val_loss: 2.4175\n",
      "Epoch [40], last_lr: 0.00001, train_loss: 0.3300, val_loss: 2.4170\n",
      "Epoch [41], last_lr: 0.00001, train_loss: 0.3391, val_loss: 2.4170\n",
      "Epoch [42], last_lr: 0.00001, train_loss: 0.3409, val_loss: 2.4170\n",
      "Epoch [43], last_lr: 0.00001, train_loss: 0.3369, val_loss: 2.4170\n",
      "Epoch [44], last_lr: 0.00001, train_loss: 0.3377, val_loss: 2.4171\n",
      "Epoch [45], last_lr: 0.00001, train_loss: 0.3406, val_loss: 2.4169\n",
      "Epoch [46], last_lr: 0.00001, train_loss: 0.3391, val_loss: 2.4169\n",
      "Epoch [47], last_lr: 0.00001, train_loss: 0.3513, val_loss: 2.4167\n",
      "Epoch [48], last_lr: 0.00001, train_loss: 0.3420, val_loss: 2.4165\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch [49], last_lr: 0.00000, train_loss: 0.3418, val_loss: 2.4165\n",
      "Epoch [50], last_lr: 0.00000, train_loss: 0.3363, val_loss: 2.4165\n",
      "Epoch [51], last_lr: 0.00000, train_loss: 0.3415, val_loss: 2.4166\n",
      "Epoch [52], last_lr: 0.00000, train_loss: 0.3249, val_loss: 2.4166\n",
      "Epoch [53], last_lr: 0.00000, train_loss: 0.3291, val_loss: 2.4165\n",
      "Epoch [54], last_lr: 0.00000, train_loss: 0.3327, val_loss: 2.4165\n",
      "Epoch [55], last_lr: 0.00000, train_loss: 0.3423, val_loss: 2.4166\n",
      "Epoch [56], last_lr: 0.00000, train_loss: 0.3345, val_loss: 2.4165\n",
      "Epoch [57], last_lr: 0.00000, train_loss: 0.3398, val_loss: 2.4165\n",
      "Epoch [58], last_lr: 0.00000, train_loss: 0.3429, val_loss: 2.4165\n",
      "Epoch [59], last_lr: 0.00000, train_loss: 0.3336, val_loss: 2.4165\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch [60], last_lr: 0.00000, train_loss: 0.3449, val_loss: 2.4165\n",
      "Epoch [61], last_lr: 0.00000, train_loss: 0.3385, val_loss: 2.4165\n",
      "Epoch [62], last_lr: 0.00000, train_loss: 0.3467, val_loss: 2.4165\n",
      "Epoch [63], last_lr: 0.00000, train_loss: 0.3370, val_loss: 2.4165\n",
      "Epoch [64], last_lr: 0.00000, train_loss: 0.3454, val_loss: 2.4165\n",
      "Epoch [65], last_lr: 0.00000, train_loss: 0.3425, val_loss: 2.4165\n",
      "Epoch [66], last_lr: 0.00000, train_loss: 0.3245, val_loss: 2.4165\n",
      "Epoch [67], last_lr: 0.00000, train_loss: 0.3301, val_loss: 2.4165\n",
      "Epoch [68], last_lr: 0.00000, train_loss: 0.3389, val_loss: 2.4165\n",
      "Epoch [69], last_lr: 0.00000, train_loss: 0.3454, val_loss: 2.4165\n",
      "Epoch [70], last_lr: 0.00000, train_loss: 0.3395, val_loss: 2.4165\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch [71], last_lr: 0.00000, train_loss: 0.3348, val_loss: 2.4165\n",
      "Epoch [72], last_lr: 0.00000, train_loss: 0.3462, val_loss: 2.4165\n",
      "Epoch [73], last_lr: 0.00000, train_loss: 0.3448, val_loss: 2.4165\n",
      "Epoch [74], last_lr: 0.00000, train_loss: 0.3434, val_loss: 2.4165\n",
      "Epoch [75], last_lr: 0.00000, train_loss: 0.3374, val_loss: 2.4165\n",
      "Epoch [76], last_lr: 0.00000, train_loss: 0.3355, val_loss: 2.4165\n",
      "Epoch [77], last_lr: 0.00000, train_loss: 0.3372, val_loss: 2.4165\n",
      "Epoch [78], last_lr: 0.00000, train_loss: 0.3313, val_loss: 2.4165\n",
      "Epoch [79], last_lr: 0.00000, train_loss: 0.3378, val_loss: 2.4165\n",
      "Epoch [80], last_lr: 0.00000, train_loss: 0.3414, val_loss: 2.4165\n",
      "Epoch [81], last_lr: 0.00000, train_loss: 0.3318, val_loss: 2.4165\n",
      "Epoch [82], last_lr: 0.00000, train_loss: 0.3428, val_loss: 2.4165\n",
      "Epoch [83], last_lr: 0.00000, train_loss: 0.3395, val_loss: 2.4165\n",
      "Epoch [84], last_lr: 0.00000, train_loss: 0.3440, val_loss: 2.4165\n",
      "Epoch [85], last_lr: 0.00000, train_loss: 0.3342, val_loss: 2.4165\n",
      "Epoch [86], last_lr: 0.00000, train_loss: 0.3378, val_loss: 2.4165\n",
      "Epoch [87], last_lr: 0.00000, train_loss: 0.3456, val_loss: 2.4165\n",
      "Epoch [88], last_lr: 0.00000, train_loss: 0.3383, val_loss: 2.4165\n",
      "Epoch [89], last_lr: 0.00000, train_loss: 0.3314, val_loss: 2.4165\n",
      "Epoch [90], last_lr: 0.00000, train_loss: 0.3422, val_loss: 2.4165\n",
      "Epoch [91], last_lr: 0.00000, train_loss: 0.3396, val_loss: 2.4165\n",
      "Epoch [92], last_lr: 0.00000, train_loss: 0.3337, val_loss: 2.4165\n",
      "Epoch [93], last_lr: 0.00000, train_loss: 0.3437, val_loss: 2.4165\n",
      "Epoch [94], last_lr: 0.00000, train_loss: 0.3375, val_loss: 2.4165\n",
      "Epoch [95], last_lr: 0.00000, train_loss: 0.3427, val_loss: 2.4165\n",
      "Epoch [96], last_lr: 0.00000, train_loss: 0.3482, val_loss: 2.4165\n",
      "Epoch [97], last_lr: 0.00000, train_loss: 0.3346, val_loss: 2.4165\n",
      "Epoch [98], last_lr: 0.00000, train_loss: 0.3463, val_loss: 2.4165\n",
      "Epoch [99], last_lr: 0.00000, train_loss: 0.3376, val_loss: 2.4165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model = LSTMnetwork(text_embedding_dimension = text_embedding_dimension).to(device)\n",
    "epochs = 100\n",
    "save_path = \"saved_models/LSTM_STSB_100epochs.pt\"\n",
    "history = fit_one_cycle(epochs, model, train_df, val_df, device = device, save_path = save_path)\n",
    "model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95014d9",
   "metadata": {},
   "source": [
    "## Creating the forget and retain sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4fa3fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_retain = train_df[train_df['score'] >= 2]\n",
    "val_df_retain = val_df[val_df['score'] >= 2]\n",
    "test_df_retain = test_df[test_df['score'] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beb1c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_forget = train_df[train_df['score'] < 2]\n",
    "val_df_forget = val_df[val_df['score'] < 2]\n",
    "test_df_forget = test_df[test_df['score'] < 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b8fe69",
   "metadata": {},
   "source": [
    "## Retraining the model from scratch on Retain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6581b7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.01000, train_loss: 4.4909, val_loss: 1.1749\n",
      "Epoch [1], last_lr: 0.01000, train_loss: 0.9404, val_loss: 1.3314\n",
      "Epoch [2], last_lr: 0.01000, train_loss: 0.7599, val_loss: 0.9330\n",
      "Epoch [3], last_lr: 0.01000, train_loss: 0.6883, val_loss: 1.0268\n",
      "Epoch [4], last_lr: 0.01000, train_loss: 0.6612, val_loss: 0.9592\n",
      "Epoch [5], last_lr: 0.01000, train_loss: 0.6337, val_loss: 1.0138\n",
      "Epoch [6], last_lr: 0.01000, train_loss: 0.6384, val_loss: 0.9921\n",
      "Epoch [7], last_lr: 0.01000, train_loss: 0.6282, val_loss: 0.9893\n",
      "Epoch [8], last_lr: 0.01000, train_loss: 0.6106, val_loss: 0.9939\n",
      "Epoch [9], last_lr: 0.01000, train_loss: 0.5835, val_loss: 1.0177\n",
      "Epoch [10], last_lr: 0.01000, train_loss: 0.5419, val_loss: 1.0217\n",
      "Epoch [11], last_lr: 0.01000, train_loss: 0.5061, val_loss: 1.0469\n",
      "Epoch [12], last_lr: 0.01000, train_loss: 0.4733, val_loss: 1.0764\n",
      "Epoch [13], last_lr: 0.01000, train_loss: 0.4352, val_loss: 1.1303\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch [14], last_lr: 0.00100, train_loss: 0.3902, val_loss: 1.0564\n",
      "Epoch [15], last_lr: 0.00100, train_loss: 0.3695, val_loss: 1.0537\n",
      "Epoch [16], last_lr: 0.00100, train_loss: 0.3652, val_loss: 1.0650\n",
      "Epoch [17], last_lr: 0.00100, train_loss: 0.3575, val_loss: 1.0767\n",
      "Epoch [18], last_lr: 0.00100, train_loss: 0.3496, val_loss: 1.0717\n",
      "Epoch [19], last_lr: 0.00100, train_loss: 0.3539, val_loss: 1.0816\n",
      "Epoch [20], last_lr: 0.00100, train_loss: 0.3503, val_loss: 1.0682\n",
      "Epoch [21], last_lr: 0.00100, train_loss: 0.3332, val_loss: 1.0863\n",
      "Epoch [22], last_lr: 0.00100, train_loss: 0.3259, val_loss: 1.0894\n",
      "Epoch [23], last_lr: 0.00100, train_loss: 0.3229, val_loss: 1.0952\n",
      "Epoch [24], last_lr: 0.00100, train_loss: 0.3161, val_loss: 1.0974\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch [25], last_lr: 0.00010, train_loss: 0.3126, val_loss: 1.0999\n",
      "Epoch [26], last_lr: 0.00010, train_loss: 0.3121, val_loss: 1.1028\n",
      "Epoch [27], last_lr: 0.00010, train_loss: 0.3107, val_loss: 1.1022\n",
      "Epoch [28], last_lr: 0.00010, train_loss: 0.3096, val_loss: 1.1047\n",
      "Epoch [29], last_lr: 0.00010, train_loss: 0.3067, val_loss: 1.1033\n",
      "Epoch [30], last_lr: 0.00010, train_loss: 0.3111, val_loss: 1.1007\n",
      "Epoch [31], last_lr: 0.00010, train_loss: 0.3119, val_loss: 1.1031\n",
      "Epoch [32], last_lr: 0.00010, train_loss: 0.3076, val_loss: 1.1046\n",
      "Epoch [33], last_lr: 0.00010, train_loss: 0.3026, val_loss: 1.1058\n",
      "Epoch [34], last_lr: 0.00010, train_loss: 0.3026, val_loss: 1.1040\n",
      "Epoch [35], last_lr: 0.00010, train_loss: 0.2972, val_loss: 1.1092\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch [36], last_lr: 0.00001, train_loss: 0.3002, val_loss: 1.1089\n",
      "Epoch [37], last_lr: 0.00001, train_loss: 0.3023, val_loss: 1.1078\n",
      "Epoch [38], last_lr: 0.00001, train_loss: 0.3008, val_loss: 1.1062\n",
      "Epoch [39], last_lr: 0.00001, train_loss: 0.3050, val_loss: 1.1055\n",
      "Epoch [40], last_lr: 0.00001, train_loss: 0.3099, val_loss: 1.1050\n",
      "Epoch [41], last_lr: 0.00001, train_loss: 0.3022, val_loss: 1.1052\n",
      "Epoch [42], last_lr: 0.00001, train_loss: 0.3069, val_loss: 1.1055\n",
      "Epoch [43], last_lr: 0.00001, train_loss: 0.3120, val_loss: 1.1049\n",
      "Epoch [44], last_lr: 0.00001, train_loss: 0.3075, val_loss: 1.1048\n",
      "Epoch [45], last_lr: 0.00001, train_loss: 0.2988, val_loss: 1.1044\n",
      "Epoch [46], last_lr: 0.00001, train_loss: 0.3010, val_loss: 1.1038\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch [47], last_lr: 0.00000, train_loss: 0.3095, val_loss: 1.1038\n",
      "Epoch [48], last_lr: 0.00000, train_loss: 0.3041, val_loss: 1.1038\n",
      "Epoch [49], last_lr: 0.00000, train_loss: 0.3057, val_loss: 1.1039\n",
      "Epoch [50], last_lr: 0.00000, train_loss: 0.2969, val_loss: 1.1039\n",
      "Epoch [51], last_lr: 0.00000, train_loss: 0.3009, val_loss: 1.1039\n",
      "Epoch [52], last_lr: 0.00000, train_loss: 0.3028, val_loss: 1.1039\n",
      "Epoch [53], last_lr: 0.00000, train_loss: 0.3057, val_loss: 1.1039\n",
      "Epoch [54], last_lr: 0.00000, train_loss: 0.3009, val_loss: 1.1038\n",
      "Epoch [55], last_lr: 0.00000, train_loss: 0.3039, val_loss: 1.1038\n",
      "Epoch [56], last_lr: 0.00000, train_loss: 0.3031, val_loss: 1.1038\n",
      "Epoch [57], last_lr: 0.00000, train_loss: 0.3043, val_loss: 1.1039\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch [58], last_lr: 0.00000, train_loss: 0.3009, val_loss: 1.1039\n",
      "Epoch [59], last_lr: 0.00000, train_loss: 0.3036, val_loss: 1.1039\n",
      "Epoch [60], last_lr: 0.00000, train_loss: 0.3057, val_loss: 1.1039\n",
      "Epoch [61], last_lr: 0.00000, train_loss: 0.3021, val_loss: 1.1039\n",
      "Epoch [62], last_lr: 0.00000, train_loss: 0.3059, val_loss: 1.1039\n",
      "Epoch [63], last_lr: 0.00000, train_loss: 0.3051, val_loss: 1.1039\n",
      "Epoch [64], last_lr: 0.00000, train_loss: 0.3073, val_loss: 1.1039\n",
      "Epoch [65], last_lr: 0.00000, train_loss: 0.3056, val_loss: 1.1039\n",
      "Epoch [66], last_lr: 0.00000, train_loss: 0.3067, val_loss: 1.1039\n",
      "Epoch [67], last_lr: 0.00000, train_loss: 0.3114, val_loss: 1.1039\n",
      "Epoch [68], last_lr: 0.00000, train_loss: 0.3039, val_loss: 1.1039\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch [69], last_lr: 0.00000, train_loss: 0.2969, val_loss: 1.1039\n",
      "Epoch [70], last_lr: 0.00000, train_loss: 0.3046, val_loss: 1.1039\n",
      "Epoch [71], last_lr: 0.00000, train_loss: 0.3028, val_loss: 1.1039\n",
      "Epoch [72], last_lr: 0.00000, train_loss: 0.3094, val_loss: 1.1039\n",
      "Epoch [73], last_lr: 0.00000, train_loss: 0.3110, val_loss: 1.1039\n",
      "Epoch [74], last_lr: 0.00000, train_loss: 0.3052, val_loss: 1.1039\n",
      "Epoch [75], last_lr: 0.00000, train_loss: 0.2993, val_loss: 1.1039\n",
      "Epoch [76], last_lr: 0.00000, train_loss: 0.3104, val_loss: 1.1039\n",
      "Epoch [77], last_lr: 0.00000, train_loss: 0.3045, val_loss: 1.1039\n",
      "Epoch [78], last_lr: 0.00000, train_loss: 0.3094, val_loss: 1.1039\n",
      "Epoch [79], last_lr: 0.00000, train_loss: 0.3047, val_loss: 1.1039\n",
      "Epoch [80], last_lr: 0.00000, train_loss: 0.3046, val_loss: 1.1039\n",
      "Epoch [81], last_lr: 0.00000, train_loss: 0.3047, val_loss: 1.1039\n",
      "Epoch [82], last_lr: 0.00000, train_loss: 0.3123, val_loss: 1.1039\n",
      "Epoch [83], last_lr: 0.00000, train_loss: 0.3108, val_loss: 1.1039\n",
      "Epoch [84], last_lr: 0.00000, train_loss: 0.3012, val_loss: 1.1039\n",
      "Epoch [85], last_lr: 0.00000, train_loss: 0.3051, val_loss: 1.1039\n",
      "Epoch [86], last_lr: 0.00000, train_loss: 0.3100, val_loss: 1.1039\n",
      "Epoch [87], last_lr: 0.00000, train_loss: 0.3029, val_loss: 1.1039\n",
      "Epoch [88], last_lr: 0.00000, train_loss: 0.3065, val_loss: 1.1039\n",
      "Epoch [89], last_lr: 0.00000, train_loss: 0.3035, val_loss: 1.1039\n",
      "Epoch [90], last_lr: 0.00000, train_loss: 0.3059, val_loss: 1.1039\n",
      "Epoch [91], last_lr: 0.00000, train_loss: 0.2980, val_loss: 1.1039\n",
      "Epoch [92], last_lr: 0.00000, train_loss: 0.3091, val_loss: 1.1039\n",
      "Epoch [93], last_lr: 0.00000, train_loss: 0.3034, val_loss: 1.1039\n",
      "Epoch [94], last_lr: 0.00000, train_loss: 0.3047, val_loss: 1.1039\n",
      "Epoch [95], last_lr: 0.00000, train_loss: 0.2994, val_loss: 1.1039\n",
      "Epoch [96], last_lr: 0.00000, train_loss: 0.2968, val_loss: 1.1039\n",
      "Epoch [97], last_lr: 0.00000, train_loss: 0.3181, val_loss: 1.1039\n",
      "Epoch [98], last_lr: 0.00000, train_loss: 0.2990, val_loss: 1.1039\n",
      "Epoch [99], last_lr: 0.00000, train_loss: 0.3063, val_loss: 1.1039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "gold_model = LSTMnetwork(text_embedding_dimension = text_embedding_dimension).to(device)\n",
    "\n",
    "epochs = 100\n",
    "save_path = \"saved_models/LSTM_STSB_100epochs_0to2_retrained.pt\"\n",
    "history = fit_one_cycle(epochs, gold_model, train_df_retain, val_df_retain, device = device, save_path = save_path)\n",
    "gold_model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b753b",
   "metadata": {},
   "source": [
    "### Evaluate the retrained model on various cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56387ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss': 2.841952850846533}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_df_retain, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16bb9939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss': 1.7932759574140893}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_df_forget, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9dd8476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss': 0.9258318761899855}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(gold_model, test_df_retain, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e51de613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss': 7.221951234429545}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(gold_model, test_df_forget, 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce5e55",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5b988cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.00100, train_loss: 0.9928, val_loss: 1.1080\n",
      "Epoch [1], last_lr: 0.00100, train_loss: 0.7369, val_loss: 1.0494\n",
      "Epoch [2], last_lr: 0.00100, train_loss: 0.7062, val_loss: 1.1080\n",
      "Epoch [3], last_lr: 0.00100, train_loss: 0.6652, val_loss: 1.0326\n",
      "Epoch [4], last_lr: 0.00100, train_loss: 0.6550, val_loss: 1.0242\n",
      "CPU times: user 2min 13s, sys: 45.9 ms, total: 2min 13s\n",
      "Wall time: 1.46 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "student_model = LSTMnetwork(text_embedding_dimension = text_embedding_dimension).to(device)\n",
    "student_model.load_state_dict(torch.load(\"saved_models/LSTM_STSB_100epochs.pt\"))\n",
    "epochs = 5\n",
    "save_path = \"saved_models/LSTM_STSB_5epochs_4to5_Finetune_Forget.pt\"\n",
    "history = fit_one_finetune_cycle(epochs, student_model, train_df_retain, val_df_retain, 0.001, device = device, save_path = save_path)\n",
    "student_model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2952073d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss': 1.0722167293689073}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(student_model, test_df_retain, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c421e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss': 5.554914107911729}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(student_model, test_df_forget, 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20853f1f",
   "metadata": {},
   "source": [
    "## Amnesiac Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b8a0585",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_df['score'].mean()\n",
    "sd = train_df['score'].std()\n",
    "\n",
    "random_preds = np.random.normal(loc=mean, scale=sd, size=(len(train_df[train_df['score'] < 2]),))\n",
    "\n",
    "amnesiac_finetune_df = train_df.copy()\n",
    "amnesiac_finetune_df.loc[amnesiac_finetune_df['score'] < 2, 'score'] = random_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c35e52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.00100, train_loss: 1.4821, val_loss: 1.3775\n",
      "Epoch [1], last_lr: 0.00100, train_loss: 1.3101, val_loss: 1.1982\n",
      "Epoch [2], last_lr: 0.00100, train_loss: 1.2878, val_loss: 1.1332\n",
      "Epoch [3], last_lr: 0.00100, train_loss: 1.2601, val_loss: 1.0947\n",
      "Epoch [4], last_lr: 0.00100, train_loss: 1.2185, val_loss: 1.0959\n",
      "CPU times: user 3min 2s, sys: 31.4 ms, total: 3min 2s\n",
      "Wall time: 1.85 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "student_model = LSTMnetwork(text_embedding_dimension = text_embedding_dimension).to(device)\n",
    "student_model.load_state_dict(torch.load(\"saved_models/LSTM_STSB_100epochs.pt\"))\n",
    "epochs = 5\n",
    "save_path = \"saved_models/LSTM_STSB_2epochs_Amnesiac_Finetune_Forget.pt\"\n",
    "history = fit_one_finetune_cycle(epochs, student_model, amnesiac_finetune_df, val_df_retain, 0.001, device = device, save_path = save_path)\n",
    "student_model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "683eca0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss': 1.2053828360775851}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(student_model, test_df_retain, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79517fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss': 4.82110405608104}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(student_model, test_df_forget, 'cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215bb9e1",
   "metadata": {},
   "source": [
    "## Blindspot Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7f20c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_train_df = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca6e4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_train_df['forget'] = 0\n",
    "u_train_df.loc[u_train_df['score'] < 2, 'forget'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667bf9a",
   "metadata": {},
   "source": [
    "### Training the Blindspot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84f6a8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.01000, train_loss: 5.3748, val_loss: 1.7657\n",
      "Epoch [1], last_lr: 0.01000, train_loss: 1.2427, val_loss: 1.0975\n",
      "Epoch [2], last_lr: 0.01000, train_loss: 0.8266, val_loss: 0.9669\n",
      "Epoch [3], last_lr: 0.01000, train_loss: 0.7312, val_loss: 0.9212\n",
      "Epoch [4], last_lr: 0.01000, train_loss: 0.7079, val_loss: 0.9279\n",
      "Epoch [5], last_lr: 0.01000, train_loss: 0.6720, val_loss: 0.9683\n",
      "Epoch [6], last_lr: 0.01000, train_loss: 0.6607, val_loss: 0.9701\n",
      "Epoch [7], last_lr: 0.01000, train_loss: 0.6526, val_loss: 0.9699\n",
      "Epoch [8], last_lr: 0.01000, train_loss: 0.6474, val_loss: 0.9719\n",
      "Epoch [9], last_lr: 0.01000, train_loss: 0.6403, val_loss: 0.9790\n",
      "CPU times: user 4min 24s, sys: 59.7 ms, total: 4min 24s\n",
      "Wall time: 2.77 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "device = 'cuda'\n",
    "proxy_model = LSTMnetwork(text_embedding_dimension = text_embedding_dimension).to(device)\n",
    "epochs = 10\n",
    "save_path = \"saved_models/LSTM_STSB_blindspot.pt\"\n",
    "history = fit_one_cycle(epochs, proxy_model, train_df_retain, val_df_retain, device = device, save_path = save_path)\n",
    "proxy_model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8b1d24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss': 0.9433637388558371}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(proxy_model, test_df_retain, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7606e738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss': 6.870501907695236}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(proxy_model, test_df_forget, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2550637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.00100, train_loss: 1.3176, val_loss: 2.6206\n",
      "Epoch [1], last_lr: 0.00100, train_loss: 0.9288, val_loss: 3.1632\n",
      "Epoch [2], last_lr: 0.00100, train_loss: 0.8149, val_loss: 3.2369\n",
      "Epoch [3], last_lr: 0.00100, train_loss: 0.7305, val_loss: 3.2416\n",
      "Epoch [4], last_lr: 0.00100, train_loss: 0.6652, val_loss: 3.2574\n",
      "CPU times: user 4min 51s, sys: 149 ms, total: 4min 51s\n",
      "Wall time: 6.44 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "AT_beta = 50\n",
    "student_model = LSTMnetwork(text_embedding_dimension = text_embedding_dimension).to(device)\n",
    "student_model.load_state_dict(torch.load(\"saved_models/LSTM_STSB_100epochs.pt\"))\n",
    "epochs = 5\n",
    "save_path = \"saved_models/LSTM_STSB_unlearn.pt\"\n",
    "history = fit_one_forget_cycle(epochs, student_model, proxy_model,  u_train_df, val_df, lr = 0.001, device = device, save_path = save_path)\n",
    "student_model.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b5f314a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss': 0.9808568328147851}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(student_model, test_df_retain, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a0a2633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Loss': 6.394269830679027}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(student_model, test_df_forget, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb568be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a3f94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
